\chapter{结语}

最后，我想引用Stuart russell和Peter Norvig的一句话：

“You could say that SVMs are successful because of one key insight, one neat trick.”

(Russell \& Norvig, 2010)

关键的观点是，有些样本比其他样本更重要。它们是最接近决策边界的，我们称它们为\textbf{支持向量}。结果表明，最优超平面比其他超平面具有更好的泛化性，并且可以只使用支持向量来构造。我们需要解决一个凸优化问题来找到这个超平面。

这个巧妙的技巧(neat trick)是\textbf{核技巧}。它允许我们对线性不可分的数据使用支持向量机，如果没有它，支持向量机将非常有限。我们发现这个技巧虽然一开始很难掌握，但实际上非常简单，可以在其他学习算法中重复使用。

就是这样。如果您已经从头到尾地阅读了这本书，那么您现在应该理解支持向量机是如何工作的了。另一个有趣的问题是为什么它们会起作用?这是一个叫做计算学习理论的领域的主题(支持向量机实际上来自于统计学习理论)。如果你想了解更多，你可以学习\href{http://work.caltech.edu/telecourse.html}{这门}优秀的课程或阅读Learning from Data(Abu-Mostafa, 2012)，它有一个很好的课程介绍。

您应该知道支持向量机不仅仅用于分类。单类支持向量机可以用于异常检测，支持向量回归（Support Vector Regression）可以用于回归。为简洁起见，本书没有将它们包括在内，但它们同样是有趣的主题。现在您已经理解了基本的支持向量机，您应该为研究这些推导做了更好的准备。

支持向量机不会解决你所有的问题，但我确实希望它们现在能成为你的机器学习工具箱中的一个工具——一个你能理解并喜欢使用的工具。